{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89766a4b-5d2a-4506-9011-aa0c2e3c9f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 IMPORT LIBRARIES AND DATASETS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False) \n",
    "# setting the style of the notebook to be monokai theme  \n",
    "# ensures that  x and y axes are visible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80eb88-61aa-4fde-b0a1-f8483862d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "tweets_df = pd.read_csv('twitter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc318a74-b17a-4bb0-bc74-e160e622f387",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee6402-e04e-42a6-b047-bcdae711e58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e96606-a416-4d68-9918-55204e68fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b822c5b0-a947-4074-a2fa-5712f88f50b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00a6c7-fc29-4063-b477-c39b80c9b7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'id' column\n",
    "tweets_df = tweets_df.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6222243-2398-4dd9-8a65-eb89cc5fd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2 PERFORM DATA EXPLORATION\n",
    "sns.heatmap(tweets_df.isnull(), yticklabels = False, cbar = False, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612aee36-4b72-4322-80d3-b1d8cc52c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.hist(bins = 30, figsize = (13,5), color = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ad273-d7f4-4aa8-83ff-86274e787c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(tweets_df['label'], label = \"Count\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced1cf7-9802-4c90-9e27-418b5741524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(tweets_df['label'], label = \"Count\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f9601-402b-4ba5-b4d5-e87919da11db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a4291-c856-47a4-adf1-fcc2a861071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c7542-5259-41a5-80b9-ac0efecc3861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the shortest message \n",
    "tweets_df[tweets_df['length'] == 11]['tweet'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc64c4-a977-440b-b14d-65f63c9fd2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 PLOT THE WORDCLOUD\n",
    "positive = tweets_df[tweets_df['label']==0]\n",
    "positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba652d2a-1311-4e89-8143-9aaa54c464a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = tweets_df[tweets_df['label']==1]\n",
    "negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7814848-dfe1-452a-945c-bbd42a4c2500",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tweets_df['tweet'].tolist()\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dedbc34-8d5c-4c3e-9468-28bbccb15dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_as_one_string =\" \".join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfb9e64-90e6-411b-9765-9ef808d5a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_as_one_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380656b-89a7-40a6-af29-0977bc9f216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2b641c-6ca9-4d23-962e-715be83e0296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(WordCloud().generate(sentences_as_one_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa48b7d-70ec-4306-a0d5-63bb2b510049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4 PERFORM DATA CLEANING - REMOVE PUNCTUATION FROM TEXT\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45405be7-4d27-4761-b4a8-46f7ba3ba8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = '$I love AI & Machine learning!!'\n",
    "Test_punc_removed = [char for char in Test if char not in string.punctuation]\n",
    "Test_punc_removed_join = ''.join(Test_punc_removed)\n",
    "Test_punc_removed_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab316a47-4e1c-488b-8813-21a6efd2db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = 'Good morning beautiful people :)... I am having fun learning Machine learning and AI!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2479f95b-df3c-4364-a5fb-8e347000246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_punc_removed = [char for char in Test if char not in string.punctuation]\n",
    "Test_punc_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72497b-22d8-43a1-a1f3-a247354992d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the characters again to form the string.\n",
    "Test_punc_removed_join = ''.join(Test_punc_removed)\n",
    "Test_punc_removed_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36302d2e-5afa-4d18-85ff-f26992c5d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 5 PERFORM DATA CLEANING - REMOVE STOPWORDS\n",
    "import nltk # Natural Language tool kit \n",
    "nltk.download('stopwords')\n",
    "\n",
    "# You have to download stopwords Package to execute this command\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe66e4eb-2324-4b30-ac9a-f16a09154d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_punc_removed_join = 'I enjoy coding, programming and Artificial intelligence'\n",
    "Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9452971e-0338-426c-96fa-cbc198cea100",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_punc_removed_join_clean # Only important (no so common) words are left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ea70a-08ec-4fc4-ba42-5760bef7f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_punc_removed_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06360ecc-0f46-40a0-b281-15d93e316714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 6 PERFORM COUNT VECTORIZATION (TOKENIZATION)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sample_data = ['This is the first paper.','This document is the second paper.','And this is the third one.','Is this the first paper?']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dab3f5-10bf-4cbb-9b58-22d42faa118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c5756-92cb-4780-af0d-0f27f3bf967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.toarray())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8653914-aca2-4fdc-95ad-4db68921d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 7 CREATE A PIPELINE TO REMOVE PUNCTUATIONS, STOPWORDS AND PERFORM COUNT VECTORIZATION\n",
    "def message_cleaning(message):\n",
    "    Test_punc_removed = [char for char in message if char not in string.punctuation]\n",
    "    Test_punc_removed_join = ''.join(Test_punc_removed)\n",
    "    Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]\n",
    "    return Test_punc_removed_join_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279f9a8-eaed-4c2f-874b-adb776295d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the newly added function\n",
    "tweets_df_clean = tweets_df['tweet'].apply(message_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc7ce2-743d-4bbc-8d6b-5dfb7765d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_df_clean[5]) # show the cleaned up version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9cf96-6d04-4e3f-8a18-055ad6475083",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_df['tweet'][5]) # show the original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7247a3-ad2d-430d-9b65-885ebe237e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Define the cleaning pipeline we defined earlier\n",
    "vectorizer = CountVectorizer(analyzer = message_cleaning, dtype = np.uint8)\n",
    "tweets_countvectorizer = vectorizer.fit_transform(tweets_df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a11b075-2529-4300-ac4f-e475207227d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd49320-efd6-48d9-90f5-0fa5816bebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_countvectorizer.toarray())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e994de0-91fe-4145-952a-61dbaa475b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_countvectorizer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38185cbb-af9c-48ff-ae67-b470576b2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(tweets_countvectorizer.toarray())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189722e-790e-4bfd-8e95-225c5614bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tweets_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f4683-bc80-432e-a753-7b47103507e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8 TRAIN AND EVALUATE A NAIVE BAYES CLASSIFIER MODEL\n",
    "X.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512813f-8859-4486-8f21-870f6b78a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ff80a-6af7-4857-8258-3ab655cece03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_classifier = MultinomialNB()\n",
    "NB_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143c2d1b-23a8-447b-abf8-15922d931666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79033d8-ce98-4c48-8a3e-937eac579211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_predict_test = NB_classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predict_test)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a549d0-02a5-484e-83b7-3ff126c99efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predict_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
